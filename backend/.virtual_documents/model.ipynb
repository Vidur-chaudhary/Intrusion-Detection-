%matplotlib inline


import subprocess
subprocess.run(["pip", "install", "pandas","pyarrow","fastparquet"])


import pandas as pd

# Load the Parquet file (replace 'your_file.parquet' with the actual filename)
df = pd.read_parquet("cic-collection.parquet", engine="pyarrow")

# Display the first few rows
df.head()



df.info()


#We are now looking and analysing the dataset
df.columns


df.isnull().sum()


df["Label"].unique()


attack_counts = df["Label"].value_counts()
print(attack_counts)



#visualizing
import matplotlib.pyplot as plt
import seaborn as sns

# Count occurrences of each attack type
attack_counts = df["Label"].value_counts()

# Plot the attack types
plt.figure(figsize=(12, 6))
sns.barplot(x=attack_counts.index, y=attack_counts.values, palette="coolwarm")
plt.xticks(rotation=90)
plt.xlabel("Attack Type")
plt.ylabel("Count")
plt.title("Number of Occurrences per Attack Type")
plt.show()


import matplotlib.pyplot as plt
import seaborn as sns

# Remove non-numeric columns
df_numeric = df.select_dtypes(include=["number"])

# Compute correlation matrix
corr_matrix = df_numeric.corr()

# Plot heatmap
plt.figure(figsize=(16, 12))  # Set figure size
sns.heatmap(corr_matrix, cmap="coolwarm", annot=False, linewidths=0.5)

# Add title
plt.title("Feature Correlation Heatmap", fontsize=14)
plt.show()



import pandas as pd

# Load the full dataset (only once at the start)
df = pd.read_parquet("cic-collection.parquet", engine="pyarrow")  # Adjust based on your actual dataset

# Define attack types for training
selected_attacks = [
    "DDoS-LOIC-HTTP", "DoS-Hulk", "Botnet",
    "Bruteforce-SSH", "Portscan", "Webattack-XSS"
]

# Create a COPY of the selected attack data (without modifying df)
df_training = df[df["Label"].isin(selected_attacks)].copy()

# Now, df_training can be used for ML training, and df remains unchanged



print(selected_attacks)



attack_counts = df_training["Label"].value_counts()
print(attack_counts)


import seaborn as sns
import matplotlib.pyplot as plt

# Count occurrences of each attack type
attack_counts = df_training["Label"].value_counts()

# Plot the bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=attack_counts.index, y=attack_counts.values, palette="viridis")

# Labeling
plt.xlabel("Attack Type")
plt.ylabel("Count")
plt.title("Distribution of Selected Attacks in Training Data")
plt.xticks(rotation=45)  # Rotate labels for readability

# Show the plot
plt.show()



#blanacing is done so that it can learn all the parameter equally
from imblearn.under_sampling import RandomUnderSampler

# Define undersampling strategy
undersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)

# Separate features and labels
X = df_training.drop(columns=["Label"])
y = df_training["Label"]

# Apply undersampling
X_resampled, y_resampled = undersample.fit_resample(X, y)

# Create a new balanced DataFrame
df_training = pd.DataFrame(X_resampled, columns=X.columns)
df_training ["Label"] = y_resampled

# Display the new class distribution
print(df_training ["Label"].value_counts())



#undersampling make the distribution same 
import matplotlib.pyplot as plt
import seaborn as sns

# Plot class distribution
plt.figure(figsize=(10, 6))
sns.countplot(data=df_training , x="Label", order=df_training ["Label"].value_counts().index, palette="viridis")

# Add labels and title
plt.xlabel("Attack Type", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.title("Class Distribution After Undersampling", fontsize=14)
plt.xticks(rotation=45, ha="right")

# Show the plot
plt.show()



print(df_training.isnull().sum())



#changing the non numeric colun to numeric 
from sklearn.preprocessing import LabelEncoder

df_training["Label"] = LabelEncoder().fit_transform(df_training["Label"])



#heatmap of the features
import seaborn as sns
import matplotlib.pyplot as plt

# Select only numerical columns
numerical_cols = df_training.select_dtypes(include=['number'])

# Compute correlation matrix
corr_matrix = numerical_cols.corr()

# Set up the figure size
plt.figure(figsize=(12, 8))

# Create a heatmap
sns.heatmap(corr_matrix, cmap="coolwarm", annot=False, linewidths=0.5)

# Set title
plt.title("Feature Correlation Heatmap")

# Show the plot
plt.show()



#checking the coerrelation

# Select only numerical columns
numerical_df = df_training.select_dtypes(include=['number'])

# Compute the correlation matrix
corr_matrix = numerical_df.corr()

# Find highly correlated features (absolute correlation > 0.9)
high_corr_features = corr_matrix.abs().unstack().sort_values(ascending=False)

# Remove self-correlation
high_corr_features = high_corr_features[high_corr_features < 1]

# Display the highly correlated feature pairs
print(high_corr_features[high_corr_features > 0.9])



import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Step 1: Plot Class Distribution
plt.figure(figsize=(10, 6))
sns.countplot(data=df_training, x="Label", order=df_training["Label"].value_counts().index, palette="viridis")

# Add labels and title
plt.xlabel("Attack Type", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.title("Class Distribution After Undersampling", fontsize=14)
plt.xticks(rotation=45, ha="right")

# Show the plot
plt.show()

# Step 2: Check Correlation
# Select only numerical columns
numerical_df = df_training.select_dtypes(include=['number'])

# Compute the correlation matrix
corr_matrix = numerical_df.corr()

# Find highly correlated features (absolute correlation > 0.9)
high_corr_features = corr_matrix.abs().unstack().sort_values(ascending=False)

# Remove self-correlation
high_corr_features = high_corr_features[high_corr_features < 1]

# Display the highly correlated feature pairs
print("Highly Correlated Features (>0.9):\n", high_corr_features[high_corr_features > 0.9])

# Step 3: Drop Highly Correlated Features
features_to_drop = set()

for feature1, feature2 in high_corr_features[high_corr_features > 0.9].index:
    features_to_drop.add(feature2)  # Drop the second feature in each pair

# Drop the selected features
df_training = df_training.drop(columns=features_to_drop)

print("Dropped Features:", features_to_drop)



# feature left after removing the highly correlated columns
selected_columns=df_training.columns
print(selected_columns)


#heatmap after highly correlated feature is removed
import seaborn as sns
import matplotlib.pyplot as plt

# Select only numerical columns
numerical_cols = df_training.select_dtypes(include=['number'])

# Compute correlation matrix
corr_matrix = numerical_cols.corr()

# Set up the figure size
plt.figure(figsize=(12, 8))

# Create a heatmap
sns.heatmap(corr_matrix, cmap="coolwarm", annot=False, linewidths=0.5)

# Set title
plt.title("Feature Correlation Heatmap")

# Show the plot
plt.show()



#splitting the dataset for training and testing 

from sklearn.model_selection import train_test_split

# Separate features (X) and target labels (y)
X = df_training.drop(columns=["Label"])  # Drop the target label column
y = df_training["Label"]  # Target variable

# Split into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training Data Shape:", X_train.shape)
print("Testing Data Shape:", X_test.shape)



# Step 1: Convert df_training into ChatGPT-friendly Prompts
import pandas as pd

# Use the existing processed DataFrame
df_training = df_training.copy()

# Selected columns (excluding Label to use it as the output)
selected_columns = [
    "Fwd Packet Length Max", "Fwd Packet Length Mean", "Fwd Packet Length Std",
    "Flow Bytes/s", "Bwd IAT Total", "Bwd IAT Std", "Bwd IAT Max",
    "Fwd PSH Flags", "Bwd Packets/s", "SYN Flag Count", "URG Flag Count",
    "Avg Fwd Segment Size", "Init Fwd Win Bytes", "Init Bwd Win Bytes",
    "Active Std", "Active Max", "Idle Std"
]

# Function to create structured text prompts
def generate_prompt(row):
    return (f"Network traffic details: "
            f"Fwd Packet Length Max = {row['Fwd Packet Length Max']}, "
            f"Fwd Packet Length Mean = {row['Fwd Packet Length Mean']}, "
            f"Fwd Packet Length Std = {row['Fwd Packet Length Std']}, "
            f"Flow Bytes/s = {row['Flow Bytes/s']}, "
            f"Bwd IAT Total = {row['Bwd IAT Total']}, "
            f"Bwd IAT Std = {row['Bwd IAT Std']}, "
            f"Bwd IAT Max = {row['Bwd IAT Max']}, "
            f"Fwd PSH Flags = {row['Fwd PSH Flags']}, "
            f"Bwd Packets/s = {row['Bwd Packets/s']}, "
            f"SYN Flag Count = {row['SYN Flag Count']}, "
            f"URG Flag Count = {row['URG Flag Count']}, "
            f"Avg Fwd Segment Size = {row['Avg Fwd Segment Size']}, "
            f"Init Fwd Win Bytes = {row['Init Fwd Win Bytes']}, "
            f"Init Bwd Win Bytes = {row['Init Bwd Win Bytes']}, "
            f"Active Std = {row['Active Std']}, "
            f"Active Max = {row['Active Max']}, "
            f"Idle Std = {row['Idle Std']}. "
            f"What is the possible network attack type? (Expected: {row['Label']})")

# Apply function to generate prompts for all rows
df_training["ChatGPT_Prompt"] = df_training.apply(generate_prompt, axis=1)

# Save to CSV for further use
df_training[["ChatGPT_Prompt", "Label"]].to_csv("chatgpt_prompts.csv", index=False)

print("✅ ChatGPT prompts generated successfully and saved to chatgpt_prompts.csv!")



pd.read_csv('chatgpt_prompts.csv')


#creating the json file for the chatgpt as it understand the json format 
import pandas as pd
import json

# Load the generated CSV file
df_prompts = pd.read_csv("chatgpt_prompts.csv")

# Rename columns to match OpenAI’s fine-tuning format
df_prompts.rename(columns={"ChatGPT_Prompt": "prompt", "Label": "completion"}, inplace=True)

# Convert DataFrame to JSONL format
jsonl_data = df_prompts.to_dict(orient='records')

# Save to JSONL file
with open("chatgpt_prompts.jsonl", "w") as f:
    for entry in jsonl_data:
        json.dump(entry, f)
        f.write("\n")

print("✅ ChatGPT training data saved as chatgpt_prompts.jsonl")



import subprocess
subprocess.run(["pip", "install", "google-generativeai"])



# this is done to see which models are working with my api key 
import google.generativeai as genai

genai.configure(api_key="AIzaSyCCP8C80gmWVUs5OnbmMHTM5a1NoSiCugo")

# List available models
models = genai.list_models()

# Print all available models
for model in models:
    print(model.name, "-", model.supported_generation_methods)



# this is done to test the api and model is working or not 
import google.generativeai as genai

# Configure API Key
genai.configure(api_key="AIzaSyCCP8C80gmWVUs5OnbmMHTM5a1NoSiCugo")

# Load the model
model = genai.GenerativeModel("gemini-1.5-pro-latest")

# Example: Generate a response from a test input
response = model.generate_content("Analyze this network attack data and predict the attack type.")

# Print response
print(response.text)



import json

# Load the JSONL dataset
data = []
with open("chatgpt_prompts.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))  # Read each line as a JSON object

# Check first few records
print(json.dumps(data[:3], indent=2))  # Print first 3 entries









#model 
'''import google.generativeai as genai
import json
import random

# Configure API Key
genai.configure(api_key="AIzaSyCCP8C80gmWVUs5OnbmMHTM5a1NoSiCugo")

# Load the Gemini model
model = genai.GenerativeModel("gemini-1.5-pro-latest")

# Load JSONL dataset
data = []
with open("chatgpt_prompts.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))

# Define a dictionary to map the labels to attack types
attack_type_mapping = {
    0: "DDoS-LOIC-HTTP",
    1: "DoS-Hulk",
    2: "Botnet",
    3: "Bruteforce-SSH",
    4: "Portscan",
    5: "Webattack-XSS"
}

# Select a few examples for few-shot learning (random 5 examples)
few_shot_examples = random.sample(data, 5)

# Prepare the few-shot prompt
few_shot_prompt = "Here are some network attack examples with their classifications:\n\n"
for example in few_shot_examples:
    few_shot_prompt += f"- {example['prompt']} Answer: {attack_type_mapping.get(example['completion'], 'Unknown')}\n\n"

# Select one test example (unseen data)
test_example = random.choice(data)

# Final Prompt for Gemini
final_prompt = few_shot_prompt + f"Now, classify this new attack:\n\n{test_example['prompt']}\nAnswer:"

# Get response from Gemini
response = model.generate_content(final_prompt)

# Print Results
print("🔹 Few-Shot Learning Prediction:")
print(f"🔸 Expected Attack Type: {attack_type_mapping.get(test_example['completion'], 'Unknown')}")
print(f"🔹 Gemini Prediction: {response.text.strip()}")'''



import google.generativeai as genai
import json
import random
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Configure API Key
genai.configure(api_key="AIzaSyCCP8C80gmWVUs5OnbmMHTM5a1NoSiCugo")

# Load the Gemini model
model = genai.GenerativeModel("gemini-1.5-pro-latest")

# Load JSONL dataset
data = []
with open("chatgpt_prompts.jsonl", "r") as file:
    for line in file:
        data.append(json.loads(line))

# Define attack type mapping
attack_type_mapping = {
    0: "DDoS-LOIC-HTTP",
    1: "DoS-Hulk",
    2: "Botnet",
    3: "Bruteforce-SSH",
    4: "Portscan",
    5: "Webattack-XSS"
}

# Select random test samples
test_samples = random.sample(data, 3)  # Adjust size as needed

# Store true and predicted labels
y_true = []
y_pred = []

# Evaluate each test sample
for sample in test_samples:
    # Ensure the model outputs only one label
    prompt = f"Classify this network attack: {sample['prompt']}\n\nReturn only one of the following labels: {', '.join(attack_type_mapping.values())}."
    
    response = model.generate_content(prompt)
    predicted_label = response.text.strip()

    # Ensure the predicted label is valid
    if predicted_label not in attack_type_mapping.values():
        print(f"Warning: Unexpected prediction '{predicted_label}'. Setting it to 'Unknown'.")
        predicted_label = "Unknown"

    # Store results
    y_true.append(attack_type_mapping.get(sample["completion"], "Unknown"))
    y_pred.append(predicted_label)

    # Print results for debugging
    print(f"Expected: {y_true[-1]}, Predicted: {y_pred[-1]}")

# Evaluation Metrics
print("\nAccuracy:", accuracy_score(y_true, y_pred))
print("\nClassification Report:")
print(classification_report(y_true, y_pred, zero_division=1))  # Avoid warnings

# Confusion Matrix
cm_labels = list(attack_type_mapping.values()) + ["Unknown"]
cm = confusion_matrix(y_true, y_pred, labels=cm_labels)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=cm_labels, yticklabels=cm_labels, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()



import pandas as pd

# Load the Parquet file
df = pd.read_parquet('cic-collection.parquet', engine='pyarrow')

# Save to CSV
df.to_csv('cic_data.csv', index=False)

print("CSV file saved as cic_data.csv")



import pandas as pd

# Load the CSV file
df = pd.read_csv('/Users/vidurchaudhary/Downloads/hciproject/backend/cic_data.csv')

# Get dimensions (rows, columns)
print(f"Dimensions: {df.shape}")
print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")




import pandas as pd

# Load your main dataset
df = pd.read_csv("/Users/vidurchaudhary/Downloads/hciproject/backend/cic_data.csv")

# Select only the important columns
selected_columns = [
    'Flow Duration',
    'Total Fwd Packets',
    'Total Backward Packets',
    'Flow Bytes/s',
    'Flow Packets/s',
    'Packet Length Mean',
    'SYN Flag Count',
    'Avg Packet Size',
    'Bwd Packet Length Mean',
    'Label'
]

# Create a new DataFrame with only selected columns
df_selected = df[selected_columns]

# Save to new CSV
df_selected.to_csv("cic_dashboard_features.csv", index=False)

